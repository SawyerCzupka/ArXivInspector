# docker compose file to run qdrant
# Recommended command: docker run -p 6333:6333 qdrant/qdrant

version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    volumes:
      - ./docker/qdrant/storage:/qdrant/storage
    restart: unless-stopped

  oobabooga:
    image: atinoda/text-generation-webui:latest # Specify variant as the :tag
    container_name: oobabooga
    environment:
      - EXTRA_LAUNCH_ARGS="--listen --verbose" # Custom launch args (e.g., --model MODEL_NAME)
#      - BUILD_EXTENSIONS_LIVE="silero_tts whisper_stt" # Install named extensions during every container launch. THIS WILL SIGNIFICANLTLY SLOW LAUNCH TIME.
    ports:
      - "7860:7860"  # Default web port
      - "5000:5000"  # Default API port
#      - "5005:5005"  # Default streaming port
      - "5001:5001"  # Default OpenAI API extension port
    volumes:
      - ./config/characters:/app/characters
      - ./config/loras:/app/loras
      - ./config/models:/app/models
      - ./config/presets:/app/presets
      - ./config/prompts:/app/prompts
      - ./config/training:/app/training
#      - ./config/extensions:/app/extensions  # Persist all extensions
#      - ./config/extensions/silero_tts:/app/extensions/silero_tts  # Persist a single extension
    logging:
      driver:  json-file
      options:
        max-file: "3"   # number of files or file count
        max-size: '10m'
    deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                device_ids: ['0']
                capabilities: [gpu]

  text-embeddings-inference:
    image: ghcr.io/huggingface/text-embeddings-inference:0.6
    container_name: text-embeddings-inference
    command:
      - --model-id=jinaai/jina-embeddings-v2-base-en
      - --max-batch-tokens=500
      - --tokenization-workers=1
    ports:
      - "8080:80"
    volumes:
      - ./docker/text-embeddings-inference/config:/config
    deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                device_ids: ['0']
                capabilities: [gpu]